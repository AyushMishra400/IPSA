{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               embed\n",
      "0  rt nowthisnews rep trey radel r fl slams obama...\n",
      "1  video obamacare full of higher costs and broke...\n",
      "2  please join me today in remembering our fallen...\n",
      "3  rt senatorleahy 1st step toward senate debate ...\n",
      "4   amazon delivery drones show need to update la...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:39: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/pc/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "fname = open(\"Political-media-DFE.csv\")\n",
    "df = pd.read_csv(fname, error_bad_lines=False, encoding=\"latin-1\")\n",
    "new_df = pd.DataFrame()\n",
    "\n",
    "def cleaner(text):\n",
    "    try:\n",
    "        text = re.sub('<[^>]*>', ' ', text)    # removes HTML from tweets\n",
    "        text = re.sub('(http|https)://[^ ]+ ', '', text)    # removes all the hyperlinks\n",
    "        text = re.sub('\\s\\s+', '', text)    # removes all the extra whitespaces\n",
    "        emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P|[^T_T])', text)    #find all emoticons\n",
    "        text = re.sub('[\\W]+', ' ', text.lower()) + ''.join(emoticons).replace('-', '')  # appends emmoticons at the end.\n",
    "    except:\n",
    "        text = text\n",
    "    try:\n",
    "        text=unicode(text)\n",
    "    except:\n",
    "        pass\n",
    "    return text\n",
    "\n",
    "new_df['embed'] = df['embed'].apply(cleaner)\n",
    "print new_df.head()\n",
    "\n",
    "def type_to_token(tag):\n",
    "\tif tag=='neutral':\n",
    "\t\treturn 0\n",
    "\telse:\n",
    "\t\treturn 1\n",
    "\n",
    "for i in range(len((df))):\n",
    "    new_df.ix[i, 'bias'] = type_to_token(df.ix[i, 'bias'])\n",
    "    \n",
    "# Processing into tokens\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "def tokenizer_porter(text):\n",
    "#     return [porter.stem(word) for word in text.split()]\n",
    "    for word in text.split():\n",
    "        try:\n",
    "            return porter.stem(word)\n",
    "        except Exception:\n",
    "            return word\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1501    speaking on the house floor tonight to honor m...\n",
      "2586     function d s id var js fjs d getelementsbytag...\n",
      "2653     function d s id var js fjs d getelementsbytag...\n",
      "1055    as a social worker i know impact a teacher can...\n",
      "705     rt robrobinson way to go nashville via usatoda...\n",
      "Name: embed, dtype: object\n",
      "Accuracy: 73.82%\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(new_df['embed'], new_df['bias'], test_size=0.33, random_state=42) \n",
    "\n",
    "print X_test.head()\n",
    "classes = np.array([0,1])\n",
    "vect = HashingVectorizer(decode_error='ignore',\n",
    "                         n_features = 2**21,\n",
    "                         preprocessor = None,\n",
    "                         tokenizer=tokenizer\n",
    "                        )\n",
    "clf = SGDClassifier(loss='log', random_state=1, n_iter=1)\n",
    "X_train = vect.transform(X_train)\n",
    "clf.partial_fit(X_train, y_train, classes=classes)\t\n",
    "\n",
    "def token_to_type(num):\n",
    "\tif num == 1:\n",
    "\t\treturn \"partial\"\n",
    "\treturn \"bias\"\n",
    "\n",
    "# print(\"The prediction : {}\".format(token_to_type(clf.predict(X_test[]))))\n",
    "X_test = vect.transform(X_test)\n",
    "print('Accuracy: {:.2f}%'.format(clf.score(X_test, y_test)*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 73.27%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "nn_clf = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(5, 2), random_state=1)\n",
    "nn_clf.fit(X_train, y_train)\n",
    "print('Accuracy: {:.2f}%'.format(nn_clf.score(X_test, y_test)*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.]\n"
     ]
    }
   ],
   "source": [
    "# print X_test[4] \n",
    "print nn_clf.predict(X_test[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Added version check for recent scikit-learn 0.18 checks\n",
    "from distutils.version import LooseVersion as Version\n",
    "from sklearn import __version__ as sklearn_version\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(new_df['embed'], new_df['bias'], test_size=0.33, random_state=42) \n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "if Version(sklearn_version) < '0.18':\n",
    "    from sklearn.grid_search import GridSearchCV\n",
    "else:\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        preprocessor=None)\n",
    "\n",
    "param_grid = [{'vect__ngram_range': [(1, 1)],\n",
    "               'vect__stop_words': [stop, None],\n",
    "               'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "               'clf__penalty': ['l1', 'l2'],\n",
    "               'clf__C': [1.0, 10.0, 100.0]},\n",
    "              {'vect__ngram_range': [(1, 1)],\n",
    "               'vect__stop_words': [stop, None],\n",
    "               'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "               'vect__use_idf':[False],\n",
    "               'vect__norm':[None],\n",
    "               'clf__penalty': ['l1', 'l2'],\n",
    "               'clf__C': [1.0, 10.0, 100.0]},\n",
    "              ]\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf),\n",
    "                     ('clf', LogisticRegression(random_state=0))])\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=5,\n",
    "                           verbose=1,\n",
    "                           n_jobs=-1,\n",
    "                           error_score=5,\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   10.5s\n",
      "[Parallel(n_jobs=-1)]: Done 240 out of 240 | elapsed:   13.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=5,\n",
       "       estimator=Pipeline(steps=[('vect', TfidfVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=False, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=Tru...nalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid=[{'vect__ngram_range': [(1, 1)], 'vect__tokenizer': [<function tokenizer at 0x7f0a78304ed8>, <function tokenizer_porter at 0x7f0a78304b18>], 'clf__penalty': ['l1', 'l2'], 'clf__C': [1.0, 10.0, 100.0], 'vect__stop_words': [[u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves...0a78304b18>], 'vect__use_idf': [False], 'clf__C': [1.0, 10.0, 100.0], 'clf__penalty': ['l1', 'l2']}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_lr_tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.48%\n"
     ]
    }
   ],
   "source": [
    "lgr_clf = gs_lr_tfidf.best_estimator_\n",
    "print('Accuracy: {:.2f}%'.format(lgr_clf.score(X_test, y_test)*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, os\n",
    "\n",
    "dest = os.path.join('pkl_objects')\n",
    "if not os.path.exists(dest):\n",
    "\tos.makedirs(dest)\n",
    "\n",
    "pickle.dump(\n",
    "\tclf, open(os.path.join(dest, 'classifier.pkl'), 'wb')\n",
    "\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
